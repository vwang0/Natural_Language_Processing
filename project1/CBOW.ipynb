{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_w2v_data  \n",
    "from visual import show_w2v_word_embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # numbers\n",
    "    \"5 2 4 8 6 2 3 6 4\",\n",
    "    \"4 8 5 6 9 5 5 6\",\n",
    "    \"1 1 5 2 3 3 8\",\n",
    "    \"3 6 9 6 8 7 4 6 3\",\n",
    "    \"8 9 9 6 1 4 3 4\",\n",
    "    \"1 0 2 0 2 1 3 3 3 3 3\",\n",
    "    \"9 3 3 0 1 4 7 8\",\n",
    "    \"9 9 8 5 6 7 1 2 3 0 1 0\",\n",
    "\n",
    "    # alphabets, expecting that 9 is close to letters\n",
    "    \"a t g q e h 9 u f\",\n",
    "    \"e q y u o i p s\",\n",
    "    \"q o 9 p l k j o k k o p\",\n",
    "    \"h g y i u t t a e q\",\n",
    "    \"i k d q r e 9 e a d\",\n",
    "    \"o p d g 9 s a f g a\",\n",
    "    \"i u y g h k l a s w\",\n",
    "    \"o l u y a o g f s\",\n",
    "    \"o p i u y g d a s j d l\",\n",
    "    \"u k i l o 9 l j s\",\n",
    "    \"y g i s h k j l f r f\",\n",
    "    \"i o h n 9 9 d 9 f a 9\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(keras.Model):\n",
    "    def __init__(self, v_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.v_dim = v_dim\n",
    "        self.embeddings = keras.layers.Embedding(\n",
    "            input_dim=v_dim, output_dim=emb_dim,  # [n_vocab, emb_dim]\n",
    "            embeddings_initializer=keras.initializers.RandomNormal(0., 0.1),\n",
    "        )\n",
    "\n",
    "        # noise-contrastive estimation\n",
    "        self.nce_w = self.add_weight(\n",
    "            name=\"nce_w\", shape=[v_dim, emb_dim],\n",
    "            initializer=keras.initializers.TruncatedNormal(0., 0.1))  # [n_vocab, emb_dim]\n",
    "        self.nce_b = self.add_weight(\n",
    "            name=\"nce_b\", shape=(v_dim,),\n",
    "            initializer=keras.initializers.Constant(0.1))  # [n_vocab, ]\n",
    "\n",
    "        self.opt = keras.optimizers.Adam(0.01)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        # x.shape = [n, skip_window*2]\n",
    "        o = self.embeddings(x)          # [n, skip_window*2, emb_dim]\n",
    "        o = tf.reduce_mean(o, axis=1)   # [n, emb_dim]\n",
    "        return o\n",
    "\n",
    "    # negative sampling: take one positive label and num_sampled negative labels to compute the loss\n",
    "    # in order to reduce the computation of full softmax\n",
    "    def loss(self, x, y, training=None):\n",
    "        embedded = self.call(x, training)\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=self.nce_w, biases=self.nce_b, labels=tf.expand_dims(y, axis=1),\n",
    "                inputs=embedded, num_sampled=5, num_classes=self.v_dim))\n",
    "\n",
    "    def step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(x, y, True)\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    for t in range(2500):\n",
    "        bx, by = data.sample(8)\n",
    "        loss = model.step(bx, by)\n",
    "        if t % 200 == 0:\n",
    "            print(\"step: {} | loss: {}\".format(t, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vocabularies sorted from more frequent to less frequent:\n",
      " ['9' '3' 'o' '6' 'a' '1' 'i' 'g' 's' '4' 'l' 'k' '8' 'u' '2' 'd' '5' 'y'\n",
      " 'f' 'e' 'h' 'p' 'q' '0' 'j' '7' 't' 'r' 'w' 'n']\n",
      "5 example pairs:\n",
      " [[16 14 12  3  9]\n",
      " [14  9  3 14 12]\n",
      " [ 9 12 14  1  3]\n",
      " [12  3  1  3 14]\n",
      " [ 3 14  3  9  1]]\n",
      "step: 0 | loss: 9.69774341583252\n",
      "step: 200 | loss: 3.2020909786224365\n",
      "step: 400 | loss: 2.5976340770721436\n",
      "step: 600 | loss: 2.523277759552002\n",
      "step: 800 | loss: 2.4113540649414062\n",
      "step: 1000 | loss: 2.293454170227051\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    d = process_w2v_data(corpus, skip_window=2, method=\"cbow\")\n",
    "    m = CBOW(d.num_word, 2)\n",
    "    train(m, d)\n",
    "\n",
    "    # plotting\n",
    "    show_w2v_word_embedding(m, d, \"./visual/results/cbow.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
